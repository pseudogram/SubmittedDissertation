{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient\n",
    "Implementation followed: Continuous control with deep reinforcement learning (arXiv:1509.02971v5)\n",
    "- Memory Relay\n",
    "- A3C\n",
    "- Trained with a target net\n",
    "- Initial exploration policy is quite important to warm up the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T17:07:25.995578\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, n_observation, n_action, name='actor_net'):\n",
    "        self.n_observation = n_observation\n",
    "        self.n_action = n_action\n",
    "        self.name = name\n",
    "        self.sess = None\n",
    "        self.build_model()\n",
    "        self.build_train()\n",
    "        \n",
    "    def build_model(self):\n",
    "        activation = tf.nn.elu\n",
    "        kernel_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "        kernel_regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "        default_dense = partial(tf.layers.dense,\\\n",
    "                                activation=activation,\\\n",
    "                                kernel_initializer=kernel_initializer,\\\n",
    "                                kernel_regularizer=kernel_regularizer)\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            observation = tf.placeholder(tf.float32,shape=[None,self.n_observation])\n",
    "            hid1 = default_dense(observation,32)\n",
    "            hid2 = default_dense(hid1,64)\n",
    "            action = default_dense(hid2,self.n_action,activation=tf.nn.tanh,use_bias=False)\n",
    "            trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=self.name)\n",
    "        self.observation,self.action,self.trainable_vars = observation,action,trainable_vars\n",
    "        \n",
    "    def build_train(self,learning_rate = 0.0001):\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            action_grads = tf.placeholder(tf.float32,[None,self.n_action])\n",
    "            var_grads = tf.gradients(self.action,self.trainable_vars,-action_grads)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).apply_gradients(zip(var_grads,self.trainable_vars))\n",
    "        self.action_grads,self.train_op = action_grads,train_op\n",
    "        \n",
    "    def predict_action(self,obs_batch):\n",
    "        return self.action.eval(session=self.sess,feed_dict={self.observation:obs_batch})\n",
    "\n",
    "    def train(self,obs_batch,action_grads):\n",
    "        batch_size = len(action_grads)\n",
    "        self.train_op.run(session=self.sess,feed_dict={self.observation:obs_batch,self.action_grads:action_grads/batch_size})\n",
    "        \n",
    "    def set_session(self,sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "    def get_trainable_dict(self):\n",
    "        return {var.name[len(self.name):]: var for var in self.trainable_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, n_observation, n_action, name='critic_net'):\n",
    "        self.n_observation = n_observation\n",
    "        self.n_action = n_action\n",
    "        self.name = name\n",
    "        self.sess = None\n",
    "        self.build_model()\n",
    "        self.build_train()\n",
    "        \n",
    "    def build_model(self):\n",
    "        activation = tf.nn.elu\n",
    "        kernel_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "        kernel_regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "        default_dense = partial(tf.layers.dense,\\\n",
    "                                activation=activation,\\\n",
    "                                kernel_initializer=kernel_initializer,\\\n",
    "                                kernel_regularizer=kernel_regularizer)\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            observation = tf.placeholder(tf.float32,shape=[None,self.n_observation])\n",
    "            action = tf.placeholder(tf.float32,shape=[None,self.n_action])\n",
    "            hid1 = default_dense(observation,32)\n",
    "            hid2 = default_dense(action,32)\n",
    "            hid3 = tf.concat([hid1,hid2],axis=1)\n",
    "            hid4 = default_dense(hid3,128)\n",
    "            Q = default_dense(hid4,1, activation=None)\n",
    "            trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=self.name)\n",
    "        self.observation,self.action,self.Q,self.trainable_vars= observation,action,Q,trainable_vars\n",
    "    \n",
    "    def build_train(self,learning_rate=0.001):\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            Qexpected = tf.placeholder(tf.float32,shape=[None,1])\n",
    "            loss = tf.losses.mean_squared_error(Qexpected,self.Q)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss)\n",
    "        self.Qexpected,self.train_op = Qexpected,train_op\n",
    "        self.action_grads = tf.gradients(self.Q,self.action)[0]\n",
    "    \n",
    "    def predict_Q(self,obs_batch,action_batch):\n",
    "        return self.Q.eval(session=self.sess,\\\n",
    "                           feed_dict={self.observation:obs_batch,self.action:action_batch})\n",
    "    \n",
    "    def compute_action_grads(self,obs_batch,action_batch):\n",
    "        return self.action_grads.eval(session=self.sess,\\\n",
    "                               feed_dict={self.observation:obs_batch,self.action:action_batch})\n",
    "    def train(self,obs_batch,action_batch,Qexpected_batch):\n",
    "        self.train_op.run(session=self.sess,\\\n",
    "                          feed_dict={self.observation:obs_batch,self.action:action_batch,self.Qexpected:Qexpected_batch})\n",
    "    \n",
    "    def set_session(self,sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "    def get_trainable_dict(self):\n",
    "        return {var.name[len(self.name):]: var for var in self.trainable_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AsyncNets(object):\n",
    "    def __init__(self,class_name):\n",
    "        class_ = eval(class_name)\n",
    "        self.net = class_(3,1,name=class_name)\n",
    "        self.target_net = class_(3,1,name='{}_target'.format(class_name))\n",
    "        self.TAU = tf.placeholder(tf.float32,shape=None)\n",
    "        self.sess = None\n",
    "        self.__build_async_assign()\n",
    "    \n",
    "    def __build_async_assign(self):\n",
    "        net_dict = self.net.get_trainable_dict()\n",
    "        target_net_dict = self.target_net.get_trainable_dict()\n",
    "        keys = net_dict.keys()\n",
    "        async_update_op = [target_net_dict[key].assign((1-self.TAU)*target_net_dict[key]+self.TAU*net_dict[key]) \\\n",
    "                           for key in keys]\n",
    "        self.async_update_op = async_update_op\n",
    "    \n",
    "    def async_update(self,tau=0.01):\n",
    "        self.sess.run(self.async_update_op,feed_dict={self.TAU:tau})\n",
    "    \n",
    "    def set_session(self,sess):\n",
    "        self.sess = sess\n",
    "        self.net.set_session(sess)\n",
    "        self.target_net.set_session(sess)\n",
    "    \n",
    "    def get_subnets(self):\n",
    "        return self.net, self.target_net\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory(object):\n",
    "    def __init__(self,memory_size=10000):\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.memory_size = memory_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def append(self,item):\n",
    "        self.memory.append(item)\n",
    "        \n",
    "    def sample_batch(self,batch_size=256):\n",
    "        idx = np.random.permutation(len(self.memory))[:batch_size]\n",
    "        return [self.memory[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def UONoise():\n",
    "    theta = 0.15\n",
    "    sigma = 0.2\n",
    "    state = 0\n",
    "    while True:\n",
    "        yield state\n",
    "        state += -theta*state+sigma*np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-22 17:07:35,412] Making new env: Pendulum-v0\n",
      "[2018-04-22 17:07:35,434] Starting new video recorder writing to /Users/dattlee/Developer/Final_Year_Project/tmp/openaigym.video.0.10474.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 190, ep 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-22 17:07:39,761] Starting new video recorder writing to /Users/dattlee/Developer/Final_Year_Project/tmp/openaigym.video.0.10474.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 199, ep 0, score -1214.675689, steps 200\n",
      "iter 399, ep 1, score -997.673005, steps 200\n",
      "iter 599, ep 2, score -1539.627618, steps 200\n",
      "iter 799, ep 3, score -1524.000772, steps 200\n",
      "iter 999, ep 4, score -1042.662316, steps 200\n",
      "iter 1199, ep 5, score -954.466750, steps 200\n",
      "iter 1399, ep 6, score -1651.210360, steps 200\n",
      "iter 1599, ep 7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-22 17:07:47,335] Starting new video recorder writing to /Users/dattlee/Developer/Final_Year_Project/tmp/openaigym.video.0.10474.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", score -1127.422672, steps 200\n",
      "iter 1799, ep 8, score -1334.673865, steps 200\n",
      "iter 1999, ep 9, score -1100.311806, steps 200\n",
      "iter 2199, ep 10, score -1608.512705, steps 200\n",
      "iter 2399, ep 11, score -1169.280587, steps 200\n",
      "iter 2599, ep 12, score -1441.181427, steps 200\n",
      "iter 2799, ep 13, score -1553.228150, steps 200\n",
      "iter 2999, ep 14, score -1214.210095, steps 200\n",
      "iter 3199, ep 15, score -1488.731149, steps 200\n",
      "iter 3399, ep 16, score -1371.057099, steps 200\n",
      "iter 3599, ep 17, score -1731.397070, steps 200\n",
      "iter 3799, ep 18, score -1692.031704, steps 200\n",
      "iter 3999, ep 19, score -871.118443, steps 200\n",
      "iter 4199, ep 20, score -1042.120064, steps 200\n",
      "iter 4399, ep 21, score -1286.671239, steps 200\n",
      "iter 4599, ep 22, score -1662.843445, steps 200\n",
      "iter 4799, ep 23, score -1569.380840, steps 200\n",
      "iter 4999, ep 24, score -1111.897616, steps 200\n",
      "iter 5199, ep 25, score -1734.783088, steps 200\n",
      "iter 5373, ep 26"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-22 17:08:07,946] Starting new video recorder writing to /Users/dattlee/Developer/Final_Year_Project/tmp/openaigym.video.0.10474.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5399, ep 26, score -1506.677845, steps 200\n",
      "iter 5599, ep 27, score -1652.145745, steps 200\n",
      "iter 5799, ep 28, score -1464.733009, steps 200\n",
      "iter 5999, ep 29, score -1501.309866, steps 200\n",
      "iter 6199, ep 30, score -1543.808192, steps 200\n",
      "iter 6399, ep 31, score -1288.773206, steps 200\n",
      "iter 6599, ep 32, score -1317.635515, steps 200\n",
      "iter 6799, ep 33, score -1398.969117, steps 200\n",
      "iter 6999, ep 34, score -1255.573886, steps 200\n",
      "iter 7199, ep 35, score -1317.657525, steps 200\n",
      "iter 7399, ep 36, score -1314.577563, steps 200\n",
      "iter 7599, ep 37, score -1265.277316, steps 200\n",
      "iter 7799, ep 38, score -1285.866488, steps 200\n",
      "iter 7999, ep 39, score -1212.146464, steps 200\n",
      "iter 8199, ep 40, score -1171.436901, steps 200\n",
      "iter 8399, ep 41, score -1283.800316, steps 200\n",
      "iter 8599, ep 42, score -1020.476557, steps 200\n",
      "iter 8799, ep 43, score -994.013213, steps 200\n",
      "iter 8999, ep 44, score -1098.952588, steps 200\n",
      "iter 9199, ep 45, score -629.126610, steps 200\n",
      "iter 9399, ep 46, score -1234.618854, steps 200\n",
      "iter 9599, ep 47, score -1166.992330, steps 200\n",
      "iter 9799, ep 48, score -994.813181, steps 200\n",
      "iter 9999, ep 49, score -1040.592467, steps 200\n",
      "iter 10199, ep 50, score -1301.816931, steps 200\n",
      "iter 10399, ep 51, score -1197.566195, steps 200\n",
      "iter 10599, ep 52, score -620.691737, steps 200\n",
      "iter 10799, ep 53, score -656.584099, steps 200\n",
      "iter 10999, ep 54, score -644.519424, steps 200\n",
      "iter 11199, ep 55, score -748.316150, steps 200\n",
      "iter 11399, ep 56, score -110.111326, steps 200\n",
      "iter 11599, ep 57, score -254.938319, steps 200\n",
      "iter 11799, ep 58, score -768.486303, steps 200\n",
      "iter 11999, ep 59, score -637.097785, steps 200\n",
      "iter 12199, ep 60, score -633.560524, steps 200\n",
      "iter 12399, ep 61, score -501.933534, steps 200\n",
      "iter 12599, ep 62, score -128.147851, steps 200\n",
      "iter 12780, ep 63"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-22 17:08:50,102] Starting new video recorder writing to /Users/dattlee/Developer/Final_Year_Project/tmp/openaigym.video.0.10474.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12799, ep 63, score -513.585037, steps 200\n",
      "iter 12999, ep 64, score -818.839757, steps 200\n",
      "iter 13199, ep 65, score -410.040974, steps 200\n",
      "iter 13399, ep 66, score -380.771165, steps 200\n",
      "iter 13599, ep 67, score -257.671634, steps 200\n",
      "iter 13799, ep 68, score -129.489024, steps 200\n",
      "iter 13999, ep 69, score -869.282968, steps 200\n",
      "iter 14199, ep 70, score -128.807583, steps 200\n",
      "iter 14399, ep 71, score -1330.224335, steps 200\n",
      "iter 14599, ep 72, score -381.995667, steps 200\n",
      "iter 14799, ep 73, score -0.390461, steps 200\n",
      "iter 14999, ep 74, score -285.611740, steps 200\n",
      "iter 15199, ep 75, score -130.103164, steps 200\n",
      "iter 15399, ep 76, score -128.114088, steps 200\n",
      "iter 15599, ep 77, score -504.963379, steps 200\n",
      "iter 15799, ep 78, score -0.135106, steps 200\n",
      "iter 15999, ep 79, score -269.577870, steps 200\n",
      "iter 16199, ep 80, score -380.923231, steps 200\n",
      "iter 16399, ep 81, score -127.849147, steps 200\n",
      "iter 16599, ep 82, score -124.345633, steps 200\n",
      "iter 16799, ep 83, score -130.584580, steps 200\n",
      "iter 16999, ep 84, score -1.364385, steps 200\n",
      "iter 17199, ep 85, score -0.528783, steps 200\n",
      "iter 17399, ep 86, score -284.292959, steps 200\n",
      "iter 17599, ep 87, score -406.906163, steps 200\n",
      "iter 17799, ep 88, score -368.457600, steps 200\n",
      "iter 17999, ep 89, score -388.811843, steps 200\n",
      "iter 18199, ep 90, score -126.621942, steps 200\n",
      "iter 18399, ep 91, score -247.504411, steps 200\n",
      "iter 18599, ep 92, score -285.800592, steps 200\n",
      "iter 18799, ep 93, score -127.081691, steps 200\n",
      "iter 18999, ep 94, score -131.366795, steps 200\n",
      "iter 19199, ep 95, score -430.601830, steps 200\n",
      "iter 19399, ep 96, score -889.160121, steps 200\n",
      "iter 19599, ep 97, score -129.577928, steps 200\n",
      "iter 19799, ep 98, score -394.976160, steps 200\n",
      "iter 19999, ep 99, score -315.324084, steps 200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of DDPG_net_Class.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Actor/Actor/dense/bias/Adam, Actor/Actor/dense/bias/Adam_1, Actor/Actor/dense/kernel/Adam, Actor/Actor/dense/kernel/Adam_1, Actor/Actor/dense_1/bias/Adam, Actor/Actor/dense_1/bias/Adam_1, Actor/Actor/dense_1/kernel/Adam, Actor/Actor/dense_1/kernel/Adam_1, Actor/Actor/dense_2/kernel/Adam, Actor/Actor/dense_2/kernel/Adam_1, Actor/dense/bias, Actor/dense/kernel, Actor/dense_1/bias, Actor/dense_1/kernel, Actor/dense_2/kernel, Actor_1/beta1_power, Actor_1/beta2_power, Actor_target/Actor_target/dense/bias/Adam, Actor_target/Actor_target/dense/bias/Adam_1, Actor_target/Actor_target/dense/kernel/Adam, Actor_target/Actor_target/dense/kernel/Adam_1, Actor_target/Actor_target/dense_1/bias/Adam, Actor_target/Actor_target/dense_1/bias/Adam_1, Actor_target/Actor_target/dense_1/kernel/Adam, Actor_target/Actor_target/dense_1/kernel/Adam_1, Actor_target/Actor_target/dense_2/kernel/Adam, Actor_target/Actor_target/dense_2/kernel/Adam_1, Actor_target/dense/bias, Actor_target/dense/kernel, Actor_target/dense_1/bias, Actor_target/dense_1/kernel, Actor_target/dense_2/kernel, Actor_target_1/beta1_power, Actor_target_1/beta2_power, Critic/Critic/dense/bias/Adam, Critic/Critic/dense/bias/Adam_1, Critic/Critic/dense/kernel/Adam, Critic/Critic/dense/kernel/Adam_1, Critic/Critic/dense_1/bias/Adam, Critic/Critic/dense_1/bias/Adam_1, Critic/Critic/dense_1/kernel/Adam, Critic/Critic/dense_1/kernel/Adam_1, Critic/Critic/dense_2/bias/Adam, Critic/Critic/dense_2/bias/Adam_1, Critic/Critic/dense_2/kernel/Adam, Critic/Critic/dense_2/kernel/Adam_1, Critic/Critic/dense_3/bias/Adam, Critic/Critic/dense_3/bias/Adam_1, Critic/Critic/dense_3/kernel/Adam, Critic/Critic/dense_3/kernel/Adam_1, Critic/dense/bias, Critic/dense/kernel, Critic/dense_1/bias, Critic/dense_1/kernel, Critic/dense_2/bias, Critic/dense_2/kernel, Critic/dense_3/bias, Critic/dense_3/kernel, Critic_1/beta1_power, Critic_1/beta2_power, Critic_target/Critic_target/dense/bias/Adam, Critic_target/Critic_target/dense/bias/Adam_1, Critic_target/Critic_target/dense/kernel/Adam, Critic_target/Critic_target/dense/kernel/Adam_1, Critic_target/Critic_target/dense_1/bias/Adam, Critic_target/Critic_target/dense_1/bias/Adam_1, Critic_target/Critic_target/dense_1/kernel/Adam, Critic_target/Critic_target/dense_1/kernel/Adam_1, Critic_target/Critic_target/dense_2/bias/Adam, Critic_target/Critic_target/dense_2/bias/Adam_1, Critic_target/Critic_target/dense_2/kernel/Adam, Critic_target/Critic_target/dense_2/kernel/Adam_1, Critic_target/Critic_target/dense_3/bias/Adam, Critic_target/Critic_target/dense_3/bias/Adam_1, Critic_target/Critic_target/dense_3/kernel/Adam, Critic_target/Critic_target/dense_3/kernel/Adam_1, Critic_target/dense/bias, Critic_target/dense/kernel, Critic_target/dense_1/bias, Critic_target/dense_1/kernel, Critic_target/dense_2/bias, Critic_target/dense_2/kernel, Critic_target/dense_3/bias, Critic_target/dense_3/kernel, Critic_target_1/beta1_power, Critic_target_1/beta2_power)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1653\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Actor/Actor/dense/bias/Adam, Actor/Actor/dense/bias/Adam_1, Actor/Actor/dense/kernel/Adam, Actor/Actor/dense/kernel/Adam_1, Actor/Actor/dense_1/bias/Adam, Actor/Actor/dense_1/bias/Adam_1, Actor/Actor/dense_1/kernel/Adam, Actor/Actor/dense_1/kernel/Adam_1, Actor/Actor/dense_2/kernel/Adam, Actor/Actor/dense_2/kernel/Adam_1, Actor/dense/bias, Actor/dense/kernel, Actor/dense_1/bias, Actor/dense_1/kernel, Actor/dense_2/kernel, Actor_1/beta1_power, Actor_1/beta2_power, Actor_target/Actor_target/dense/bias/Adam, Actor_target/Actor_target/dense/bias/Adam_1, Actor_target/Actor_target/dense/kernel/Adam, Actor_target/Actor_target/dense/kernel/Adam_1, Actor_target/Actor_target/dense_1/bias/Adam, Actor_target/Actor_target/dense_1/bias/Adam_1, Actor_target/Actor_target/dense_1/kernel/Adam, Actor_target/Actor_target/dense_1/kernel/Adam_1, Actor_target/Actor_target/dense_2/kernel/Adam, Actor_target/Actor_target/dense_2/kernel/Adam_1, Actor_target/dense/bias, Actor_target/dense/kernel, Actor_target/dense_1/bias, Actor_target/dense_1/kernel, Actor_target/dense_2/kernel, Actor_target_1/beta1_power, Actor_target_1/beta2_power, Critic/Critic/dense/bias/Adam, Critic/Critic/dense/bias/Adam_1, Critic/Critic/dense/kernel/Adam, Critic/Critic/dense/kernel/Adam_1, Critic/Critic/dense_1/bias/Adam, Critic/Critic/dense_1/bias/Adam_1, Critic/Critic/dense_1/kernel/Adam, Critic/Critic/dense_1/kernel/Adam_1, Critic/Critic/dense_2/bias/Adam, Critic/Critic/dense_2/bias/Adam_1, Critic/Critic/dense_2/kernel/Adam, Critic/Critic/dense_2/kernel/Adam_1, Critic/Critic/dense_3/bias/Adam, Critic/Critic/dense_3/bias/Adam_1, Critic/Critic/dense_3/kernel/Adam, Critic/Critic/dense_3/kernel/Adam_1, Critic/dense/bias, Critic/dense/kernel, Critic/dense_1/bias, Critic/dense_1/kernel, Critic/dense_2/bias, Critic/dense_2/kernel, Critic/dense_3/bias, Critic/dense_3/kernel, Critic_1/beta1_power, Critic_1/beta2_power, Critic_target/Critic_target/dense/bias/Adam, Critic_target/Critic_target/dense/bias/Adam_1, Critic_target/Critic_target/dense/kernel/Adam, Critic_target/Critic_target/dense/kernel/Adam_1, Critic_target/Critic_target/dense_1/bias/Adam, Critic_target/Critic_target/dense_1/bias/Adam_1, Critic_target/Critic_target/dense_1/kernel/Adam, Critic_target/Critic_target/dense_1/kernel/Adam_1, Critic_target/Critic_target/dense_2/bias/Adam, Critic_target/Critic_target/dense_2/bias/Adam_1, Critic_target/Critic_target/dense_2/kernel/Adam, Critic_target/Critic_target/dense_2/kernel/Adam_1, Critic_target/Critic_target/dense_3/bias/Adam, Critic_target/Critic_target/dense_3/bias/Adam_1, Critic_target/Critic_target/dense_3/kernel/Adam, Critic_target/Critic_target/dense_3/kernel/Adam_1, Critic_target/dense/bias, Critic_target/dense/kernel, Critic_target/dense_1/bias, Critic_target/dense_1/kernel, Critic_target/dense_2/bias, Critic_target/dense_2/kernel, Critic_target/dense_3/bias, Critic_target/dense_3/kernel, Critic_target_1/beta1_power, Critic_target_1/beta2_power)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-cdb76787f645>\", line 19, in <module>\n    saver = tf.train.Saver()\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1293, in __init__\n    self.build()\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1302, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1339, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 793, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 326, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 241, in save_op\n    tensors)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1173, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/dattlee/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Actor/Actor/dense/bias/Adam, Actor/Actor/dense/bias/Adam_1, Actor/Actor/dense/kernel/Adam, Actor/Actor/dense/kernel/Adam_1, Actor/Actor/dense_1/bias/Adam, Actor/Actor/dense_1/bias/Adam_1, Actor/Actor/dense_1/kernel/Adam, Actor/Actor/dense_1/kernel/Adam_1, Actor/Actor/dense_2/kernel/Adam, Actor/Actor/dense_2/kernel/Adam_1, Actor/dense/bias, Actor/dense/kernel, Actor/dense_1/bias, Actor/dense_1/kernel, Actor/dense_2/kernel, Actor_1/beta1_power, Actor_1/beta2_power, Actor_target/Actor_target/dense/bias/Adam, Actor_target/Actor_target/dense/bias/Adam_1, Actor_target/Actor_target/dense/kernel/Adam, Actor_target/Actor_target/dense/kernel/Adam_1, Actor_target/Actor_target/dense_1/bias/Adam, Actor_target/Actor_target/dense_1/bias/Adam_1, Actor_target/Actor_target/dense_1/kernel/Adam, Actor_target/Actor_target/dense_1/kernel/Adam_1, Actor_target/Actor_target/dense_2/kernel/Adam, Actor_target/Actor_target/dense_2/kernel/Adam_1, Actor_target/dense/bias, Actor_target/dense/kernel, Actor_target/dense_1/bias, Actor_target/dense_1/kernel, Actor_target/dense_2/kernel, Actor_target_1/beta1_power, Actor_target_1/beta2_power, Critic/Critic/dense/bias/Adam, Critic/Critic/dense/bias/Adam_1, Critic/Critic/dense/kernel/Adam, Critic/Critic/dense/kernel/Adam_1, Critic/Critic/dense_1/bias/Adam, Critic/Critic/dense_1/bias/Adam_1, Critic/Critic/dense_1/kernel/Adam, Critic/Critic/dense_1/kernel/Adam_1, Critic/Critic/dense_2/bias/Adam, Critic/Critic/dense_2/bias/Adam_1, Critic/Critic/dense_2/kernel/Adam, Critic/Critic/dense_2/kernel/Adam_1, Critic/Critic/dense_3/bias/Adam, Critic/Critic/dense_3/bias/Adam_1, Critic/Critic/dense_3/kernel/Adam, Critic/Critic/dense_3/kernel/Adam_1, Critic/dense/bias, Critic/dense/kernel, Critic/dense_1/bias, Critic/dense_1/kernel, Critic/dense_2/bias, Critic/dense_2/kernel, Critic/dense_3/bias, Critic/dense_3/kernel, Critic_1/beta1_power, Critic_1/beta2_power, Critic_target/Critic_target/dense/bias/Adam, Critic_target/Critic_target/dense/bias/Adam_1, Critic_target/Critic_target/dense/kernel/Adam, Critic_target/Critic_target/dense/kernel/Adam_1, Critic_target/Critic_target/dense_1/bias/Adam, Critic_target/Critic_target/dense_1/bias/Adam_1, Critic_target/Critic_target/dense_1/kernel/Adam, Critic_target/Critic_target/dense_1/kernel/Adam_1, Critic_target/Critic_target/dense_2/bias/Adam, Critic_target/Critic_target/dense_2/bias/Adam_1, Critic_target/Critic_target/dense_2/kernel/Adam, Critic_target/Critic_target/dense_2/kernel/Adam_1, Critic_target/Critic_target/dense_3/bias/Adam, Critic_target/Critic_target/dense_3/bias/Adam_1, Critic_target/Critic_target/dense_3/kernel/Adam, Critic_target/Critic_target/dense_3/kernel/Adam_1, Critic_target/dense/bias, Critic_target/dense/kernel, Critic_target/dense_1/bias, Critic_target/dense_1/kernel, Critic_target/dense_2/bias, Critic_target/dense_2/kernel, Critic_target/dense_3/bias, Critic_target/dense_3/kernel, Critic_target_1/beta1_power, Critic_target_1/beta2_power)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cdb76787f645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUONoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/pendulum_v0_py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1673\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[1;32m   1674\u001b[0m                   save_path))\n\u001b[0;32m-> 1675\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of DDPG_net_Class.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "max_episode = 500\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "memory_warmup = batch_size*3\n",
    "max_explore_eps = 100\n",
    "save_path = 'DDPG_net_Class.ckpt'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "actorAsync = AsyncNets('Actor')\n",
    "actor,actor_target = actorAsync.get_subnets()\n",
    "criticAsync = AsyncNets('Critic')\n",
    "critic,critic_target = criticAsync.get_subnets()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    actorAsync.set_session(sess)\n",
    "    criticAsync.set_session(sess)\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    env = wrappers.Monitor(env,'./tmp/',force=True)\n",
    "    obs = env.reset()\n",
    "    iteration = 0\n",
    "    episode = 0\n",
    "    episode_score = 0\n",
    "    episode_steps = 0\n",
    "    noise = UONoise()\n",
    "    memory = Memory(memory_size)\n",
    "    while episode < max_episode:\n",
    "        print('\\riter {}, ep {}'.format(iteration,episode),end='')\n",
    "        action = actor.predict_action(np.reshape(obs,[1,-1]))[0]\n",
    "        if episode<max_explore_eps: # exploration policy\n",
    "            p = episode/max_explore_eps\n",
    "            action = action*p + (1-p)*next(noise)\n",
    "        action *= 2 # scale action\n",
    "        next_obs, reward, done,info = env.step(action)\n",
    "        memory.append([obs,action,reward,next_obs,done])\n",
    "        if iteration >= memory_warmup:\n",
    "            memory_batch = memory.sample_batch(batch_size)\n",
    "            extract_mem = lambda k : np.array([item[k] for item in memory_batch])\n",
    "            obs_batch = extract_mem(0)\n",
    "            action_batch = extract_mem(1)\n",
    "            reward_batch = extract_mem(2)\n",
    "            next_obs_batch = extract_mem(3)\n",
    "            done_batch = extract_mem(4)\n",
    "            action_next = actor_target.predict_action(next_obs_batch)\n",
    "            Q_next = critic_target.predict_Q(next_obs_batch,action_next)[:,0]\n",
    "            Qexpected_batch = reward_batch + gamma*(1-done_batch)*Q_next # target Q value\n",
    "            Qexpected_batch = np.reshape(Qexpected_batch,[-1,1])\n",
    "            # train critic\n",
    "            critic.train(obs_batch,action_batch,Qexpected_batch)\n",
    "            # train actor\n",
    "            action_grads = critic.compute_action_grads(obs_batch,action_batch)\n",
    "            actor.train(obs_batch,action_grads)\n",
    "            # async update\n",
    "            actorAsync.async_update(tau)\n",
    "            criticAsync.async_update(tau)\n",
    "        episode_score += reward\n",
    "        episode_steps += 1\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            print(', score {:8f}, steps {}'.format(episode_score,episode_steps))\n",
    "#             if episode%5 == 0:\n",
    "                \n",
    "#                 Q_check = \n",
    "            obs = env.reset()\n",
    "            episode += 1\n",
    "            episode_score = 0\n",
    "            episode_steps = 0\n",
    "            noise = UONoise()\n",
    "            if episode%100==0:\n",
    "                saver.save(sess,save_path)\n",
    "        else:\n",
    "            obs = next_obs\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-26 21:34:01,863] [Pendulum-v0] Uploading 500 episodes of training data\n",
      "[2017-08-26 21:34:03,713] [Pendulum-v0] Uploading videos of 8 training episodes (628501 bytes)\n",
      "[2017-08-26 21:34:05,040] [Pendulum-v0] Creating evaluation object from ./tmp/ with learning curve and training video\n",
      "[2017-08-26 21:34:05,260] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on Pendulum-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_ZVyGQYhVTb67h0Vu6UtOYQ\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "gym.upload('./tmp/', api_key='sk_BlwjttPKR6ZsXVrObENYA')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pendulum_v0_py3]",
   "language": "python",
   "name": "conda-env-pendulum_v0_py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
